{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 5, 5\n",
    "plt.rc(\"font\", size=10)\n",
    "\n",
    "\n",
    "plt.rc('xtick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('xtick.major', size=8, pad=12)\n",
    "plt.rc('xtick.minor', size=8, pad=12)\n",
    "\n",
    "plt.rc('ytick', color='k', labelsize='medium', direction='in')\n",
    "plt.rc('ytick.major', size=8, pad=12)\n",
    "plt.rc('ytick.minor', size=8, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have fitted our curves and doing so we have found the  best model explaining the point that we had for the fitting.\n",
    "\n",
    "We also saw that we could choose different models according to how much the improvement obtained was worth the complexification of the model. But again we did it on the whole data that we had. We never really check how well our data was generalizing to points never seen before, or by how much the model we found was subject to outliers.\n",
    "\n",
    "Machine learning procedures allows us to take those considerations into account. After highlighting the few caveats of the procedures we have used in the former notebook, we will introduce the foundation of machine learning way to model.\n",
    "\n",
    "More particularly we will see that machine learning paradigm modifies the function to optimize that we have seen before by adding a penalty to covariables that generalize badly. We will also see that in machine learning procedure, the generalization is approached by fitting and evaluating mutliple times your model on subset of your data.\n",
    "\n",
    "**Machine learning paradigm emphasizes the importance of building a general model that will be good at dealing with future unknown data points rather than being the best model on the data that we have now.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Content: <a id=\"toc\"></a>\n",
    "\n",
    "* [**Exploring model generalization in previous methods**](#explore-model-general)\n",
    "    * [**Model sensibility to a few particular points.**](#particular-points)\n",
    "    * [**How does the model change according to random data subsamples.**](#random-subsamples)\n",
    "    * [**Splitting data and regularization.**](#split-reg)\n",
    "* [**Regularization in the case of OLS and GLM with binomial distribution.**](#reg)\n",
    "    * [**Definition.**](#regularization-definition)\n",
    "    * [**Oversimplified example on Kyphosis dataset.**](#reg-kyphosis)\n",
    "* [**The machine learning framework.**](#ML-framework)\n",
    "* [**Logistic regression and OLS regression.**](#lr-ols)\n",
    "    * [**A toy model to visualize logistic regression.**](#toy-example-lr)\n",
    "    * [**A new metric to evaluate your model : AUC_ROC.**](#auc_roc)\n",
    "    * [**Imbalanced dataset.**](#imbalanced)\n",
    "    * [**Your first classical machine learning pipeline.**](#lr-ols-pipeline)\n",
    "        * [**On a classification problem using logistic regression.**](#lr-pipeline)\n",
    "        * [**On a regression problem using OLS regression.**](#ols-pipeline)\n",
    "    * [**What if you want to use GLM with regularization and cross fold validation, other than binomial?**](#glm)\n",
    "* [**A few words on scaling.**](#scaling)\n",
    "* [**A few words on leakage.**](#leakage)\n",
    "* [**Exercise - logistic regression.**](#exo-lr)\n",
    "* [**Other loss function but same regularization : Support Vector Machine.**](#svm)\n",
    "    * [**Classification.**](#svm-c)\n",
    "        * [**Formal introduction.**](#formal-svm-c)\n",
    "        * [**Toy example to visualize SVMC.**](#toy-example-svm-c)\n",
    "        * [**SVMC pipeline.**](#svm-c-pipeline)\n",
    "    * [**Regression.**](#svm-r)\n",
    "* [**Decision tree modeling : a new? loss function and new ways to do regularization.**](#decision-tree)\n",
    "    * [**Simple decision tree for classification.**](#simple-tree-c)\n",
    "        * [**Toy example to visualize decision tree.**](#toy-decision-tree)\n",
    "        * [**Single decision tree pipeline.**](#single-tree-pipeline)\n",
    "    * [**Random Forest in classification**](#rf-c)\n",
    "    * [**Random Forest in regression**](#rf-r)\n",
    "* [**Exo : regression**](#exo-regression)\n",
    "* [**Annexes**](#annex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring model generalization in previous methods. <a class=\"anchor\" id=\"explore-model-general\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model sensibility to a few particular points. <a class=\"anchor\" id=\"particular-points\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Human_nuisance.csv\")\n",
    "y=np.array(df[\"Breeding density(individuals per ha)\"])\n",
    "X=np.array(df[\"Number of pedestrians per ha per min\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(X,y,'ro')\n",
    "ax.set_ylabel(\"Breeding density(individuals per ha)\")\n",
    "ax.set_xlabel(\"Number of pedestrians per ha per min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the two last points. We could argue that they look fishy since they are the only two points that go up. Maybe they are driving the cubic fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(X[:-2],y[:-2],'ro')\n",
    "ax.set_ylabel(\"Breeding density(individuals per ha)\")\n",
    "ax.set_xlabel(\"Number of pedestrians per ha per min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from utils import make_summary_tables\n",
    "\n",
    "list_co=[]#list of covariable\n",
    "\n",
    "df=pd.read_csv(\"Human_nuisance.csv\")\n",
    "df_nuisance=pd.DataFrame()\n",
    "df_nuisance[\"Breeding\"]=df[\"Breeding density(individuals per ha)\"][:-2] \n",
    "\n",
    "ordered_loglike_multi=[]\n",
    "\n",
    "for i in range(1,5):\n",
    "\n",
    "    df_nuisance[\"N\"+\"^\"+str(i)]=df[\"Number of pedestrians per ha per min\"][:-2]**i\n",
    "    \n",
    "    list_co.append( \"N\"+\"^\"+str(i) )\n",
    "    y=df_nuisance[\"Breeding\"]\n",
    "    X= df_nuisance[ list_co ]  #again making it an array \n",
    "    X = sm.add_constant(X)#the model doesn't include an intercept automatically so we creat one\n",
    "    model = sm.OLS(y, X)# we create the least square fit object\n",
    "    results = model.fit()#we do the actual fit\n",
    "    \n",
    "    result_general_df , result_fit_df = make_summary_tables( results.summary() )\n",
    "   \n",
    "    #print(results.summary())\n",
    "    \n",
    "    ordered_loglike_multi.append( [ '_'.join(list_co) , results.llf , result_fit_df])\n",
    "    \n",
    "ordered_loglike_multi=sorted(ordered_loglike_multi,key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats \n",
    "p_value_threshold=1#0.05## what you would consider as significant or not : i.e if you will consider\n",
    "#adding the parameter or not\n",
    "\n",
    "ordered_log_name=[v[0] for v in ordered_loglike_multi][::-1]\n",
    "ordered_log_value=[v[1] for v in ordered_loglike_multi][::-1]\n",
    "\n",
    "for i in range(1,len(ordered_log_value)):#we are going through the list of models following their order in goodness of fit\n",
    "    pval=1-stats.chi2.cdf(2*(ordered_log_value[i]-ordered_log_value[i-1]),1)#calculating the pvalue\n",
    "    #that compares the goodness of fit of the two models, here we are lucky the comparison always involves two models\n",
    "    #that only differ by one parameter added, hence the df in the stats.chi.cdf for which the value is one.\n",
    "    if pval<p_value_threshold:\n",
    "        print(\"The log likelihood difference between model {0} and model {1} \\n is associated to a P value={2}\".format(ordered_log_name[i-1],ordered_log_name[i],pval))\n",
    "        print()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the choice between quadratic and cubic is borderline here....\n",
    "\n",
    "Let's check how the model behaves on fitting the dataset used for fitting and on those two points now missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df=pd.read_csv(\"Human_nuisance.csv\")\n",
    "y=np.array(df[\"Breeding density(individuals per ha)\"])\n",
    "X=np.array(df[\"Number of pedestrians per ha per min\"])\n",
    "\n",
    "## all points but the last 2\n",
    "\n",
    "#X_predicted_3=[-90.4906+49.1160*v-2.5977*v**2+0.0344*v**3 for v in X[:-2]]\n",
    "X_predicted_3 = [ ordered_loglike_multi[1][-1].loc[ 'const' , 'coef' ] + \\\n",
    "                ordered_loglike_multi[1][-1].loc[ 'N^1' , 'coef' ] * v + \\\n",
    "                ordered_loglike_multi[1][-1].loc[ 'N^2' , 'coef' ] * v**2 + \\\n",
    "                ordered_loglike_multi[1][-1].loc[ 'N^3' , 'coef' ] * v**3 for v in X[:-2]]\n",
    "\n",
    "\n",
    "#X_predicted_2=[-32.5194+29.7840*v-1.0398*v**2 for v in X[:-2]]\n",
    "X_predicted_2 = [ ordered_loglike_multi[2][-1].loc[ 'const' , 'coef' ] + \\\n",
    "                ordered_loglike_multi[2][-1].loc[ 'N^1' , 'coef' ] * v + \\\n",
    "                ordered_loglike_multi[2][-1].loc[ 'N^2' , 'coef' ] * v**2 for v in X[:-2]]\n",
    "\n",
    "## computing goodness of fit metrics \n",
    "R2_3=r2_score(y[:-2],X_predicted_3)\n",
    "MSE_3=mean_squared_error(y[:-2],X_predicted_3)\n",
    "R2_2=r2_score(y[:-2],X_predicted_2)\n",
    "MSE_2=mean_squared_error(y[:-2],X_predicted_2)\n",
    "\n",
    "\n",
    "## last 2 points\n",
    "X_predicted_32 = [ ordered_loglike_multi[1][-1].loc[ 'const' , 'coef' ] + \\\n",
    "                ordered_loglike_multi[1][-1].loc[ 'N^1' , 'coef' ] * v + \\\n",
    "                ordered_loglike_multi[1][-1].loc[ 'N^2' , 'coef' ] * v**2 + \\\n",
    "                ordered_loglike_multi[1][-1].loc[ 'N^3' , 'coef' ] * v**3 for v in X[-2:]]\n",
    "\n",
    "X_predicted_22 = [ ordered_loglike_multi[2][-1].loc[ 'const' , 'coef' ] + \\\n",
    "                ordered_loglike_multi[2][-1].loc[ 'N^1' , 'coef' ] * v + \\\n",
    "                ordered_loglike_multi[2][-1].loc[ 'N^2' , 'coef' ] * v**2 for v in X[-2:]]\n",
    "\n",
    "R2_32=r2_score(y[-2:],X_predicted_32)\n",
    "MSE_32=mean_squared_error(y[-2:],X_predicted_32)\n",
    "R2_22=r2_score(y[-2:],X_predicted_22)\n",
    "MSE_22=mean_squared_error(y[-2:],X_predicted_22)\n",
    "\n",
    "\n",
    "## plotting \n",
    "\n",
    "fig, ax = plt.subplots(2, 1,figsize=(10,10))\n",
    "\n",
    "ax[0].plot(X[:-2],y[:-2] , marker = 'o' , linestyle='None' , color = 'teal' )\n",
    "ax[0].plot(X[-2:],y[-2:] , marker = 'o' , linestyle='None' , color = 'orange' )\n",
    "ax[0].plot(X[:-2],X_predicted_2 ,'r-')\n",
    "ax[0].plot(X[-3:] , [ X_predicted_2[-1] ] + X_predicted_22 ,'r--')\n",
    "ax[0].set_title('Quadratic\\nall but 2 : R2={0:.2f}, MSE={1:.2f}\\n last 2 : R2={2:.2f}, MSE={3:.2f}'.format(R2_2,MSE_2,R2_22,MSE_22),fontsize=13)\n",
    "\n",
    "ax[1].plot(X[:-2],y[:-2] , marker = 'o' , linestyle='None' , color = 'teal' )\n",
    "ax[1].plot(X[-2:],y[-2:] , marker = 'o' , linestyle='None' , color = 'orange' )\n",
    "ax[1].plot(X[:-2],X_predicted_3 ,'r-')\n",
    "ax[1].plot(X[-3:] , [ X_predicted_3[-1] ] + X_predicted_32 ,'r--')\n",
    "ax[1].set_title('Cubic\\nall but 2 : R2={0:.2f}, MSE={1:.2f}\\n last 2 : R2={2:.2f}, MSE={3:.2f}'.format(R2_3,MSE_3,R2_32,MSE_32),fontsize=13)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cubic function is still overall better even on the points not used for the fitting (we actually kind of expected that).\n",
    "\n",
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## How does the model change according to random data subsamples. <a class=\"anchor\" id=\"random-subsamples\"></a>\n",
    "\n",
    "We should check if this kind of behaviour where it becomes difficult to assert a good model is general or is it just because we decided to get rid of those two particular points. Let's check with more random subsamples and something a bit more balanced between number of points for fitting and for checking : here two is bit low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for rememberance let's fit all the data as we did before\n",
    "\n",
    "*Note : to run the code below you need to install pydotplus (!pip install pydotplus) if you don't have it already*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import poly_fit\n",
    "\n",
    "poly_fit(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import poly_fit_train_test\n",
    "\n",
    "#poly_fit_train_test(X,y,seed,deg, ax = None)\n",
    "#return R2_train, R2_test\n",
    "\n",
    "cubic=[]\n",
    "quadratic=[]\n",
    "for i in range(3):# here we are fitting our model and checking it on different random subsample of the data \n",
    "    seed = np.random.randint( 1492 )\n",
    "    fig, ax = plt.subplots( 1 , 2 , figsize=(10,5) )\n",
    "    \n",
    "    cubic_metrics = poly_fit_train_test(X.reshape(-1,1),y, seed = seed, deg = 3 , ax = ax[0])#this contain the fit and some scoring metric\n",
    "    quad_metrics  = poly_fit_train_test(X.reshape(-1,1),y, seed = seed, deg = 2 , ax = ax[1])\n",
    "    \n",
    "    cubic.append(cubic_metrics)\n",
    "    quadratic.append(quad_metrics)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the R2 that we get between cubic and quadratic for many splitting of the dataset and for known and unknown data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic=[]\n",
    "quadratic=[]\n",
    "for i in range(500):# same as before but on way more different split \n",
    "    seed = np.random.randint( 149217 )\n",
    "    temp3=poly_fit_train_test(X.reshape(-1,1),y,seed,3,ax=None)\n",
    "    temp2=poly_fit_train_test(X.reshape(-1,1),y,seed,2,ax=None)\n",
    "    if min(temp3)>0 and min(temp2)>0:\n",
    "        cubic.append(temp3)\n",
    "        quadratic.append(temp2)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2,figsize=(10,5))\n",
    "ax[0].hist([[v[0] for v in cubic],[v[0] for v in quadratic]],label=['cubic','quadratic'])\n",
    "ax[0].set_title('Known')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_xlabel('R2')\n",
    "\n",
    "ax[1].hist([[v[1] for v in cubic],[v[1] for v in quadratic]],label=['cubic','quadratic'])\n",
    "ax[1].set_title('New')\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].set_xlabel('R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that in most of those random cases the cubic model gives better prediction on the new data points. Yet all the outcome of those fitting are a bit different. How do we reconcile them?\n",
    "\n",
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Splitting data and regularization. <a class=\"anchor\" id=\"split-reg\"></a>\n",
    "\n",
    "**Maybe then, what you would like to do is to find the model that is best at predicting new data point whatever the specific data you fit on is.** You don't want to underfit neither overfit and start modeling the noise of your data. You need to find a compromize. You will sometime hear people use the terms biais variance problem or the curse of dimensionality when refering to that problem.\n",
    "\n",
    "The approach used for that is a mix of what's called regularization, and splitting of your dataset. Regularization, as its name indicates has the ambition to smoothen your fit, to make sure that you don't start to fit the noise in your data so you can be as general in your prediction as possible.  It does that by putting another layer of constraints on your covariables (features). That constraint on your covariable translates in either the objective function you want to maximize/minimize (by adding a term in your least square or your maximum likelihood), or by constraining the space of available models.\n",
    "\n",
    "Wathever that regularization is, its strength is alway optimized by looking at subsamples of the dataset.\n",
    "\n",
    "It is a nice automated way for model exploration, generalization and testing, which for me really defines machine learning. All of this is related to something called the curse of dimensionality. **And in any case, it relies on a splitting of your data set between at least a train and a test set**.\n",
    "\n",
    "![presentation1](image/Presentation1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need the test set to assess the actual generalization of your model. **This test set should not be touched until the evaluation of your model.** Ideally by then you are looking at a model which is both good on the train and the test set.\n",
    "\n",
    "You can imagine that it is the noise that makes the coefficient in front of the 149th polynomial look very important (so big), because here by construction we know that a fit with a polynomial greater than 3 is going to fit the noise. So you should penalize big coefficients unless they are absolutely necessary. Here necessary is to be understood as necessary for understanding all the subsamples of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the ToC](#toc)\n",
    "\n",
    "# Regularization in the case of OLS and GLM with binomial distribution <a class=\"anchor\" id=\"reg\"></a>\n",
    "\n",
    "## Definition <a class=\"anchor\" id=\"regularization-definition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a Least Square fitting, you just have to add to your sum of squared errors a function that takes into account the parameters in front of your covariables. Looking at those equations you penalize weights that will take too much importance in the fitting, unless they are important in every substet of data that you fit on. We will see how those subsets are designed later on. By evaluating this new loss function on many subsets of the data we can perfom model comparison and choose model generalization, all at once. \n",
    "\n",
    "$\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}|\\beta_{j}|$ , **l1 regularization** (Lasso) $\\alpha$ being the weight that you put on that regularization \n",
    "\n",
    "$\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}\\beta_{j}^{2}$ , **l2 regularization** (Ridge) \n",
    "\n",
    "$\\sum_i (y_i-f(\\pmb X_i,\\pmb{\\beta}))^2 + \\alpha\\sum_{j}(\\alpha|\\beta_{j}|+(1-\\alpha)\\beta_{j}^{2})$ , **elasticnet**\n",
    "\n",
    "For a deeper understanding of those notions, you may look at :\n",
    "\n",
    " * https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n",
    "\n",
    " * https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "\n",
    "\n",
    "In case of a logistic regression you want to maximize your log likelihood which is now penalized by one of those functions:\n",
    "\n",
    "$\\sum_{i}log(p_{i}) - \\alpha\\sum_{j}|\\beta_{j}|$ , **l1 regularization** (Lasso) \n",
    "\n",
    "$\\sum_{i}log(p_{i}) - \\alpha\\sum_{j}\\beta_{j}^{2}$ , **l2 regularization** (Ridge) \n",
    "\n",
    "$\\sum_{i}log(p_{i}) - \\alpha\\sum_{j}(l_{1 ratio}|\\beta_{j}|+(1-l_{1 ratio})\\beta_{j}^{2})$ , **elasticnet**\n",
    "\n",
    "Rule is : **when you hypothesize that you have sparse features and so you believe that among all those features only a small subset is going to be interesting (but of course you don't know which one...) then you try to use the regularization that will tend to put more of your features at the zero weight (the l1 regularization) and so reduce the complexity of your model.** This l1 norm that collapses non important features to zero is another way to do feature selection.\n",
    "\n",
    "\n",
    "Now, we need a way to find this coefficient $\\alpha$ which will set the strength of our regularization. This parameter is called an **hyperparameter**, and cannot be found directly like the others, since even if it is part of a new model it serve a generalization purpose and so should not be found by optimization on our full dataset. To do that on top of our first splitting between train dataset and test dataset, we will need to perfom some more splitting of our train data set.\n",
    "\n",
    "> Note : the polynomial number we were using before is also an hyperparameter and can be find by the same technic consisting of splitting our data. Later on we will see other hyperparameters that are either related to model choice or regularization or intrically both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Oversimplified example on Kyphosis dataset <a class=\"anchor\" id=\"reg-kyphosis\"></a>\n",
    "\n",
    "Let's check what this regularization does for our kyphosis dataset, considering only age and number as covariables, and only one split of the data into train and test.\n",
    "\n",
    "We will, for now, focus on one particular metric to evaluate our model : **accuracy**. \n",
    "\n",
    "Accuracy just calculates the fraction of modeled labels that are the same than the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kyphosis=pd.read_csv('kyphosis.csv')\n",
    "y=[1 if v==\"present\" else 0 for v in df_kyphosis[\"Kyphosis\"]]\n",
    "df_kyphosis.drop(\"Kyphosis\",1,inplace=True)\n",
    "df_kyphosis.drop(\"Start\",1,inplace=True)\n",
    "df_kyphosis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_lr_kypho\n",
    "\n",
    "#fitting a logistic regression or a GLM Bernouilli (classifier)\n",
    "\n",
    "countour_lr_kypho(np.array(df_kyphosis),y,df_kyphosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import countour_lr_kypho_train_test\n",
    "\n",
    "#countour_lr_kypho_train_test(df,y,seed,p='l2',c=10**8,plot=True):\n",
    "#return accuracy_score(y_train, y_pred_train_c),accuracy_score(y_test, y_pred_test_c)\n",
    "\n",
    "alpha_acc=[]\n",
    "print(\"inverses of alpha\", ' , '.join([str(x) for x in np.logspace(1,-1,4) ] ) )  \n",
    "for p in np.logspace(1,-1,4):#exploring different strength of regularization (different alphas)\n",
    "    tt=countour_lr_kypho_train_test(df_kyphosis,y,0,c=p,plot=True)\n",
    "    alpha_acc.append(tt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_acc=[]\n",
    "xx=np.logspace(3,-3,200)##different strength of regularization\n",
    "for p in xx:#exploring different strength of regularization\n",
    "    tt=countour_lr_kypho_train_test(df_kyphosis,y,0,c=p,plot=False)\n",
    "    alpha_acc.append(tt)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize=(5,5))\n",
    "ax.plot(1./xx,[v[0] for v in alpha_acc],'k-',label='train')\n",
    "ax.plot(1./xx,[v[1] for v in alpha_acc],'r-',label='test')\n",
    "#ax.plot([1,1],[0.55,0.75],'k--')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with the strength of the regularization increasing both train and test accuracy decrease (at some point at least). But we can also see that there is a couple of $\\alpha$ for which the accuracy on the train set is not too much perturbed but the accuracy on the test set is greatly increased. Here some regularization seems to be advantageous to find a model that is more general. As a note, having a training score that is very high, like 0.99, should only be considered ok if the test score is high enough too. Else you are overfitting. You need to find the sweet spot which is here quite obvious.\n",
    "\n",
    "Please note that here the regularization strength is only tested on one subsample of the data, which is the test set. This is not how you should proceed. It is just a visual example. If you pick your hyperparameter because it is the best on the test set then you biased it. You should pick it according to a validation set or a cross-validation set, and only when the model is fully trained with the hyperparameters that you chose, assess the generalizability of the model by using the test set.\n",
    "\n",
    "Indeed if we would proceed like that we would be finding the best $\\alpha$ for a particular test set. This is biasing our understanding of the generalization of our model.\n",
    "\n",
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# The machine learning framework <a class=\"anchor\" id=\"ML-framework\"></a>\n",
    "\n",
    "There is actually two ways to handle this task. \n",
    "Either you have enough data and **you split your train set, once and for all, into a train and a validation set**. Or you do what's called a **cross-validation**. \n",
    "\n",
    "**Cross-validation is going to split your train set into $k$ subparts** and you are going to train your model on $k-1$ subparts pooled together and validate it on the remaining subpart. \n",
    "You do that for all the possible pairs of $k-1$ subparts. You **keep the strength of your regularization** which is on average the best. \n",
    "\n",
    "**Either way you keep your test set untouched until the end whatever happens.**\n",
    "\n",
    "![presentation3](image/Presentation3.png)\n",
    "\n",
    "![presentation4](image/Presentation4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want you to realize that the model is going to be fitted 3 times and evaluated 3 times. This is going to be 3 different model, since the fitting depends on the data...\n",
    "\n",
    "Moreover we will use that kind of multiple slpitting for hyperparameters tuning. In the case of alpha above for example, we tried 200 of those alpha so that's already 3x200 models to fit and evaluate. Imagine now we have other hyperparameters like the type of regularization , for example l1 or l2.... So you see where that is going right?\n",
    "\n",
    "This kind of strategy should be done on medium scale data or you are in for a long long fitting.\n",
    "\n",
    "\n",
    "Finally after you averaged the score you keep the hyperparameters set for which this score was the higher, and retrain on the full training set.\n",
    "\n",
    "![presentation2](image/Presentation2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those kind of methods to work, since we are solicitting our dataset many times, you need to have quite a lot of points. Typically the number of points for our sparrow disturbance is too small to perform nicely this procedure. Same for the kyphosis dataset. In particular, in this last case, the problem comes from the fact that the dataset is unbalanced (only 17 present kyphosis).\n",
    "\n",
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Logistic regression and OLS regression. <a class=\"anchor\" id=\"lr-ols\"></a>\n",
    "## A toy model to visualize logistic regression. <a class=\"anchor\" id=\"toy-example-lr\"></a>\n",
    "\n",
    "So for the next examples, let's work on clear simulated data to explore more easily how to proceed with that Machime Learning job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we want to create a model able to separate those two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "#making toy data\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=600, centers=[[-0.5,-0.5],[1,1]],cluster_std=[[1,1],[1,1]], random_state=6)\n",
    "\n",
    "\n",
    "plt.scatter(X2[:,0],X2[:,1],c=y2)\n",
    "plt.xlim(min(X2[:,1]),max(X2[:,1]))\n",
    "plt.ylim(min(X2[:,1]),max(X2[:,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try it with logistic regression and regularization l2 for different strength of regularization $\\alpha$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_lr\n",
    "\n",
    "#countour_lr(regularization,X,y,regularization strength,mult)\n",
    "\n",
    "alpha = 1/10000\n",
    "countour_lr('l2',X2,y2, 1 / alpha ,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10\n",
    "countour_lr('l2',X2,y2,1. / alpha,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 100\n",
    "countour_lr('l2',X2,y2, 1/alpha,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10000\n",
    "countour_lr('l2',X2,y2,1/alpha,'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the strength of regularization higher makes the boundary fuzzier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## A new metric to evaluate your model : AUC_ROC <a class=\"anchor\" id=\"auc_roc\"></a>\n",
    "\n",
    "\n",
    "From how we introduced logistic regression before you can already see that all points of an hyperplane share the same probability. What is represented in the first panel is one particular hyperplane (here a line in dimension 2) **for which the probability to be part of one of the two groups is =0.5**. But actually that **is quite an arbitrary threshold**. The second panel of plots shows you **many of those different hyperplanes having different probabilities and you could in principle choose any of those lines for class delimitation.**\n",
    "\n",
    "**But why would you do that? Well for example if you want to tune how stringent you want to be towards different error types.** If you deal with deciding if the patient you have in front of you should go for more exams because of a suspicion of malignant tumor, you don't want to miss any of those malignant tumors. It doesn't really matter if in the process you send many clean patient for further exams. So maybe in that case having a 0.5 threshold for malignant tumor calling is too stringent.\n",
    "\n",
    "**A way to visualize those type of problem is to look at the receiver operating curve (ROC). In this curve each point is a particular threshold and its coordinate are how much False positive and True positive this threshold will create.**\n",
    "\n",
    "$FPR= \\frac{FP}{FP+TN}$\n",
    "\n",
    "$TPR= \\frac{TP}{TP+FN}$\n",
    "\n",
    "FPR : False Positive Rate, TPR : True Positive Rate, TP: True Positive, TN: True Negative, FP: False Positive, FN: False Negative.\n",
    "**Ideally you want a curve that perfectly follows the edge of the square** : whatever threshold you use you recover all the positive and the negative samples. So the best model should have an area under the curve of 1.  In reality the curve that you will get ressemble more to the one from the third panel.\n",
    "\n",
    "There is one more thing that I would like to add here, which makes this story about ROC curves and caring about the right metric even more important. **The diagonal line in the ROC curve represents what is called a dummy classifier. It is a classifier that has learned only the probabilities to have one class instead of the other. The farther you are from that line the better you are (so the more area under the curve)**. Those comparisons to dummy classifier are really important when your dataset is imbalanced. Imagine the extreme case of a dataset made of 99 class 0 and 1 class 1. In that case your basal accuracy is going to be 98% : but that doesn't make your model good. It just means that it has chosen randomly 99 point to class as 0 and 1 to class as 1. **It is a bad model eventhough it has a high accuracy. The ROC AUC is sensitive to that**. So if you have imbalanced dataset use scoring that are sensitive to it. There are other ways to deal with it like weighting labels differently in the loss function. We will spend a little bit more time on the subject next. Just know that this problem exists and understand what it does through later on examples. Always check if your dataset is imbalanced or not and use some classifier that have built in functions to face that: logistic regression and SVC for example offer the use of the class_weight argument.\n",
    "\n",
    "**Micro exercise:** On the ROC curve can you find in which direction the thresholds are getting bigger?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Imbalanced dataset <a class=\"anchor\" id=\"imbalanced\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making toy imbalanced dataset\n",
    "X2_i, y2_i = make_blobs(n_samples=(1000,100), centers=[[1,1],[1,1]], random_state=0)\n",
    "countour_lr('l2',X2_i,y2_i,100,'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see here the data was not separable from the beginning**. But just because the dataset was imbalanced (1000 of one class and only 100 of the other) and we trained our model on accuracy score **we still get a good accuracy**. But this is missleading, our model is not usefull right now, it is randomly assigning labels according to the statistics of the labels. Again, accuracy is informative but doesn't hold all the information.\n",
    "\n",
    "\n",
    "If you don't feel like going through different scoring (i.e. accuracy,auc, f1 and so on), you can compare your model to a dummy classifier that will either only learn the statistic of your training set or always answer the most probable class (other dummy classifier are available): `sklearn.dummy.DummyClassifier`.\n",
    "\n",
    "You might have been able to deal with this problem by reweigthing the importance of the class in the model. Logistic regression provided by scikitlearn allows that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Your first classical machine learning pipeline <a class=\"anchor\" id=\"lr-ols-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On a classification problem using logistic regression. <a class=\"anchor\" id=\"lr-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a logistic regression model that will be able to predict if a breast tumor is malignant or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "#loading the dataset which is comprised in scikit learn already\n",
    "data = load_breast_cancer()\n",
    "\n",
    "\n",
    "X2=data['data']\n",
    "y2=data['target']\n",
    "\n",
    "#making it into a dataframe\n",
    "print(\"Features/covariables\\n\\t-\", '\\n\\t- '.join(data[\"feature_names\"] ) )\n",
    "breast_cancer_df=pd.DataFrame([[X2[v][u] for u in range(len(X2[v]))]for v in range(len(X2))],columns=data[\"feature_names\"])\n",
    "\n",
    "breast_cancer_df[\"target\"]=data['target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just want to point out here that all those covariables / features are defined on very different scale, for them to be treated fairly in their comparison you need to take that into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_feature={}\n",
    "for i in range(X2.shape[1]):\n",
    "    plt.hist(X2[:,i])\n",
    "    std_feature[data[\"feature_names\"][i]]=np.std(X2[:,i])\n",
    "    plt.title(data[\"feature_names\"][i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#split your data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2,y2,\n",
    "                                                   random_state=0,stratify=y2)\n",
    "#stratify is here to make sure that you split keeping the repartition of labels unaffected\n",
    "\n",
    "print(\"fraction of class benign in train\",sum(y_train)/len(y_train))\n",
    "print(\"fraction of class benign in test\",sum(y_test)/len(y_test) )\n",
    "print(\"fraction of class benign in full\",sum(y2)/len(y2))\n",
    "\n",
    "#define the score that you want to optimize\n",
    "sco='accuracy'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#create your logistic regression object, the class being slightly unbalanced add a class weight\n",
    "logi_r=LogisticRegression(class_weight='balanced')\n",
    "\n",
    "\n",
    "\n",
    "#put it in a pipeline : the pipeline allows you to put tasks to perfom in a sequential manner.\n",
    "#Here particularly to scale subset of your data at a time when you will use the cross\n",
    "#validation technique. By doing the scaling on each subset that is going for validation instead of \n",
    "#on the full training set\n",
    "#you make sure that information about your test and validation are not leaking in your training.\n",
    "#Scaling is important for some optimizers, generally speaking for technics other than logistic\n",
    "#regression or decision tree, when you add a Lasso or Ridge regularization,\n",
    "#when dealing with covariables that have a variety of scales, \n",
    "# and finally I believe it makes model intepretation easier.\n",
    "\n",
    "pipeline_lr=Pipeline([('scalar',StandardScaler()),('model',logi_r)])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "grid_values = {'model__C': np.logspace(-5,2,100),'model__penalty':['l1','l2'],'model__solver':['liblinear']}\n",
    "# define the hyperparameters you want to test\n",
    "#with the range over which you want it to be tested. Note the model double underscore name of the parameters.\n",
    "\n",
    "\n",
    "#Feed it to the GridSearchCV with the right score(here sco) over which the decision should be taken\n",
    "grid_lr_acc = GridSearchCV(pipeline_lr, param_grid = grid_values, scoring=sco,cv=5,n_jobs=-1)\n",
    "\n",
    "grid_lr_acc.fit(X_train, y_train)#train your pipeline\n",
    "\n",
    "y_decision_fn_scores_acc=grid_lr_acc.score(X_test,y_test)# calculate the score of your trained pipeline on the test\n",
    "\n",
    "print('Grid best parameter (max.'+sco+'): ', grid_lr_acc.best_params_)#get the best parameters\n",
    "print('Grid best score ('+sco+'): ', grid_lr_acc.best_score_)#best score calculated from the train/validation sets\n",
    "print('Grid best parameter (max.'+sco+') model on test: ', \n",
    "      y_decision_fn_scores_acc) #equivalent score on the test set -> this is the important metric\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_test_c = grid_lr_acc.predict(X_test)#predict y_test from X_test thanks to your trained model\n",
    "\n",
    "confusion_mc_c = confusion_matrix(y_test, y_pred_test_c)# check the number of mistake made with the default \n",
    "# threshold for your decision function\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_mc_c)\n",
    "df_cm_c = pd.DataFrame(confusion_mc_c, \n",
    "                     index = [i for i in range(2)], columns = [i for i in range(2)])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(df_cm_c, annot=True)\n",
    "plt.title('LogReg C:'+str(grid_lr_acc.best_params_['model__C'])[:5]\n",
    "          +' , norm:'+grid_lr_acc.best_params_['model__penalty']+'\\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, \n",
    "                                                                       y_pred_test_c)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_test, y_pred_test_c))# check the overall capacity of your model on test set \n",
    "#according to a bunch of metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "# this three lines here are how you get the area under the ROC curve score\n",
    "# which is very important for evaluating your model\n",
    "\n",
    "# 1. decision_function gives you the proba for a point to be in a class\n",
    "y_score_lr_c = grid_lr_acc.decision_function(X_test)\n",
    "#print(y_score_lr_c)\n",
    "\n",
    "# 2. this calculates the ROC curve\n",
    "fpr_lr_c, tpr_lr_c, thre = roc_curve(y_test, y_score_lr_c)\n",
    "\n",
    "# 3. finally this calculates the area under the curve\n",
    "roc_auc_lr_c = auc(fpr_lr_c, tpr_lr_c)\n",
    "print(\"Area under your ROC curve\",roc_auc_lr_c)\n",
    "\n",
    "#this part is just to check where your probability threshold of 0.5 sit on the roc curve\n",
    "proba=sc.special.expit(thre)\n",
    "for i in range(len(proba)):\n",
    "    if abs(proba[i]-0.5)<0.1:\n",
    "        keep=i\n",
    "        break\n",
    "        \n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_lr_c, tpr_lr_c, lw=3, label='LogRegr ROC curve\\n (area = {:0.2f})'.format(roc_auc_lr_c))\n",
    "plt.plot(fpr_lr_c[keep], tpr_lr_c[keep],'ro',label='threshold=0.5')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (logistic classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# The problem with GridSearchCV is that it doesn't give you access to the actual weights in front of your features.\n",
    "# We have to manually retrain our model with best hyperparameters found in GridSearchCV\n",
    "\n",
    "scaler = StandardScaler()# Create a scaler instance \n",
    "scaler.fit(X_train)# Fit only on train\n",
    "X_train_scaled=scaler.transform(X_train)# actual transform your data into scaled data\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(penalty=grid_lr_acc.best_params_['model__penalty'],\n",
    "                        C=grid_lr_acc.best_params_['model__C'],\n",
    "                        solver=grid_lr_acc.best_params_['model__solver'])\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "w=lr.coef_[0]#get the weights\n",
    "\n",
    "sorted_features=sorted([[data[\"feature_names\"][i],abs(w[i])] for i in range(len(w))],\n",
    "                       key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for feature, weight in sorted_features:\n",
    "    print('\\t{: <25}\\t{:.3f}'.format(feature,weight) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those are the other scores (other than accuracy and roc_auc) to evaluate the performence of your model.\n",
    "#If you recall we used scoring=accuracy in our GridSearchCV. But we could use another metric.\n",
    "\n",
    "from sklearn.metrics.scorer import SCORERS\n",
    "\n",
    "for scorer in  sorted(list(SCORERS.keys())):\n",
    "    print(scorer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On a regression problem using OLS regression and regularization . <a class=\"anchor\" id=\"ols-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a regression problem this time instead of a classification. For that we are going to build a model that is, to some extent, able to predict evolution of diabetes from some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# we remove the column sex for now\n",
    "notSexIndex = [i for i in range(len(diabetes.feature_names)) if diabetes.feature_names[i] != 'sex']\n",
    "\n",
    "df_diabetes=pd.DataFrame([ [ diabetes['data'][j,i] for i in notSexIndex ] for j in range(diabetes['data'].shape[0]) ] ,\n",
    "                         columns=  [diabetes['feature_names'][i] for i in notSexIndex ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_diabetes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#always split your dataset and do the fitting on the training\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(np.array(df_diabetes), \n",
    "                                                                                        np.array(diabetes['target']),\n",
    "                                                                                        random_state=0)\n",
    "#You will then check the goodness of fit on the test\n",
    "\n",
    "lr_reg=SGDRegressor()# This is a special function for OLS with regularization: \n",
    "# it uses Stochastic gradient Descent for its optimization algo. \n",
    "# You can change the loss function and the learning rate. \n",
    "# But we will not mess with that here. The learning rate adjustement is made\n",
    "# cleverly by default(many ways to be clever see API description) and we will stick to the MSE loss.\n",
    "\n",
    "sco=\"r2\"\n",
    "\n",
    "#in our model we will also consider polynomials for our features, thanks to that PolynomialFeature function\n",
    "#more details in the in depth notebook\n",
    "pipeline_lr_reg=Pipeline([('poly',PolynomialFeatures()),('scalar',StandardScaler()),\n",
    "                     ('model',lr_reg)])#poly first then scalar!!!\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "grid_values = {'poly__degree': np.arange(1,4,1),'model__penalty':['l1','l2'],'model__alpha':np.logspace(0,3,10)}# define the hyperparameters you want to test\n",
    "#with the range over which you want it to be tested.Here the hyperparamters are the degree of the polynomial you want to try\n",
    "# and the form of regularization you want to try (l1 or l2).Here alpha is the weight for the regularization\n",
    "\n",
    "grid_lr_reg_acc = GridSearchCV(pipeline_lr_reg, param_grid = grid_values, scoring=sco, n_jobs=-1)#Feed it to the GridSearchCV with the right\n",
    "#score over which the decision should be taken (ir R^2). This part choose for you what is the best polynomial degree you should \n",
    "#use as well as the best regularization and strength of regularization\n",
    "\n",
    "grid_lr_reg_acc.fit(X_diabetes_train, np.array(y_diabetes_train).ravel())##Where the actual fit happens\n",
    "\n",
    "y_decision_fn_scores_acc=grid_lr_reg_acc.score(X_diabetes_test,np.array(y_diabetes_test).ravel())\n",
    "\n",
    "print('Grid best parameter (max.'+sco+'): ', grid_lr_reg_acc.best_params_)#get the best parameters\n",
    "print('Grid best score ('+sco+'): ', grid_lr_reg_acc.best_score_)#get the best score calculated from the train/validation\n",
    "#dataset\n",
    "print('Grid best parameter (max. '+sco+') model on test: ', y_decision_fn_scores_acc)# get the equivalent score on the test\n",
    "#dataset : again this is the important metric\n",
    "\n",
    "\n",
    "poly=PolynomialFeatures(degree=grid_lr_reg_acc.best_params_['poly__degree'])\n",
    "X_train_poly=poly.fit_transform(X_diabetes_train)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train_poly)\n",
    "X_train_poly_scaled=scaler.transform(X_train_poly)\n",
    "\n",
    "lr_reg=SGDRegressor(penalty=grid_lr_reg_acc.best_params_['model__penalty'],alpha=grid_lr_reg_acc.best_params_['model__alpha'])\n",
    "lr_reg.fit(X_train_poly_scaled,np.array(y_diabetes_train).ravel())\n",
    "poly.powers_\n",
    "\n",
    "sorted_features=sorted([['_'.join([df_diabetes.columns[j]+'^'+str(poly.powers_[i][j])for j in range(len(df_diabetes.columns)) if poly.powers_[i][j]>0]),abs(lr_reg.coef_[i])] for i in range(len(poly.powers_))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Important features')\n",
    "\n",
    "for feature, weight in sorted_features:\n",
    "    print('\\t{: <10}\\t{:.3f}'.format(feature,weight) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## What if you want to use GLM with regularization and cross fold validation, other than binomial? <a class=\"anchor\" id=\"glm\"></a>\n",
    "\n",
    "Scikit learn have implemented GLM regression. The way it is implemented is using a minimization of the deviance modified with a L2 regularization:\n",
    "\n",
    " * https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html#sklearn.linear_model.TweedieRegressor\n",
    " * https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few words on scaling <a class=\"anchor\" id=\"scaling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is a really important part of the analysis. Some regressors or classifiers techniques are distance based (for example OLS, or whenever you directly use distances or when you are dealing with a covariance matrix transformation). Also when you use one of the regularization seen before (l1, l2 or elastic net), you should use scaling as the weights could bear uninformative scaling. If youd don't put all the features (covariables) on the same scale some will be artificially privileged because of the typical distance they represent rather than what they really mean compared to other features. \n",
    "\n",
    "Scaling also facilitates the convergence of some optimizers. Finally I personally like it because it makes my interperetation straightforward.\n",
    "\n",
    "There exists many different ways to scale features. Some were done for you in the last notebook, under the hood of some of the API function we were using. For example OLS and GLM were doing a L2 normalization on your covariables. In the examples above we used a standard scaler. Let's briefly list those.\n",
    "\n",
    "`sklearn.preprocessing.Normalizer`: you can choose your norm eitheir l1, l2 or max :\n",
    "\n",
    " L1 normalization for covariables $X_{:,j}$$$ \\frac{X_{:,j}}{\\sum_{i}|X_{i,j}|} $$\n",
    " L2 normalization for covariables $X_{:,j}$$$ \\frac{X_{:,j}}{\\sqrt{\\sum_{i}X_{i,j}^2}} $$\n",
    " max normalization for covariables $X_{:,j}$$$ \\frac{X_{:,j}}{max_i(X_{i,j})} $$\n",
    "\n",
    "\n",
    "You could also use the z variable kind of scaling: \n",
    "\n",
    "$$ \\frac{X_{:,j}-\\bar{X_{:,j}}}{\\sigma_j} $$\n",
    "\n",
    "\n",
    "Or use a min max scaler `sklearn.preprocessing.MinMaxScaler`\n",
    "\n",
    "Of course you should choose the scaling you use according to the statistic of your data. If the min and the max of your data are kind of outliers then a min max method is probably not the right choice. Those outliers will also influence greatly the l1 or l2 normalization. \n",
    "\n",
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# A few VERY IMPORTANT words on leakage. <a class=\"anchor\" id=\"leakage\"></a>\n",
    "\n",
    "The most important part in all of the machine learning jobs that we have been presenting above, is that the data set on which you train and the data set on which you evaluate your model (either is the validation set when you do hyperparameter tunning, or test set for the final evaluation) should be clearly separated. No information directly coming from your test or your validation should pollute your train set. If it does you loose your ablity to have a meaningful evaluation power. \n",
    "\n",
    "In general **data leakage** relates to every bits of information that you should not have access to in a real case scenario, being present in your training set.\n",
    "\n",
    "Among those examples of data leakage you could count : inclusion of future data points in a time dependent or event dependent model. For example, if you want to predict if a treatment should be administrated to a patient thanks to an RNAseq measurement, your model can not be developped on data recorded during the drug treatment. Obvious right? *More on that in the final exercise of this course*.\n",
    "\n",
    "Less obvious maybe, is to note that you should scale (or do other transformation based on some statistics: i.e. PCA) only on the training set or its different iteration in the cross-fold validation search. Hence the need for the pipeline. Indeed if you do a z-normalization on the full data set and only then you split into training and test, your transformed training is going to contain information about the full dataset like its mean and variance...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Exercise : logistic regression <a class=\"anchor\" id=\"exo-lr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framingham dataset links some patient features to their risk to develop a heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heart=pd.read_csv('data/framingham.csv')\n",
    "\n",
    "df_heart.dropna(axis=0,inplace=True) # removing rows with NA values.\n",
    "\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a logistic regression pipeline to predict the column `'TenYearCHD'` (dependent variable : ten year risk of coronary heart disease) by adapting some of the code above.\n",
    "\n",
    "Assess the accuracy or/and AUC_ROC of your model, and list the features by order of their importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - setup and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -19 solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - separate between train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 20-29 solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - pipeline creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 32-63 solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 65-90 solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - plotting the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 91-114 solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - getting feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 115- solutions/solution_03_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Other loss function but same regularization : Support Vector Machine <a class=\"anchor\" id=\"svm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification <a class=\"anchor\" id=\"svm-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal introduction <a class=\"anchor\" id=\"formal-svm-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle of SVM is pretty simple. SVM aims at finding the 'good' threshold (hyperplane) to separate data from different classes. Conceptually it is very different from logistic regression where you maximize the log likelihood of the log odds function. **With SVM you really look for an hyperplane that separates data and that's it : there is no underlying hypothesis about probability distribution or anything else. It is very geometrical.**\n",
    "\n",
    "You can imagine that there is quite a lot of hyperplanes separating data in your training set. You could stick your threshold right where the class 0 point closest to class 1 lies. But in that case it will be very far from the other class 0 points, which can be a problem. **You could decide that your threshold should be right between the two closest extreme of your classes but that is going to be very sensitive to missclassified data or extreme events... The points chosen as a reference to put your threshold are called Support Vectors.** \n",
    "\n",
    "So, once again, you are confronted to a compromise. You should place your threshold somwhere that is globally best even though that would mean some miss-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dummy dataset \n",
    "norm1=0.2*np.random.randn(100)-2\n",
    "norm2=0.8*np.random.randn(100)+2.5\n",
    "\n",
    "plt.plot(norm1,[1 for i in range(100)],'ro',markersize=10)\n",
    "plt.plot(norm2,[1 for i in range(100)],'bo')\n",
    "s_2=sorted(norm2)\n",
    "s_1=sorted(norm1)\n",
    "plt.plot([s_2[0],s_2[0]],[0.95,1.05],'k--',label='defined by the most extreme blue point')\n",
    "plt.plot([(s_2[0]-s_1[-1])/2+s_1[-1],(s_2[0]-s_1[-1])/2+s_1[-1]],[0.95,1.05],'k-.',label='middle of extreme of two classes')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the two hyperplanes are valid separation but you can imagine that the plane define by the most extreme blue point doesn't leave much space for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dummy dataset non linearly separable\n",
    "cauch=0.8*np.random.standard_cauchy(10)-2\n",
    "norm=1*np.random.randn(100)+2.5\n",
    "\n",
    "plt.plot(cauch,[1 for i in range(10)],'ro',markersize=10)\n",
    "plt.plot(norm,[1 for i in range(100)],'bo')\n",
    "sort_c=sorted(cauch)\n",
    "sort_n=sorted(norm)\n",
    "\n",
    "plt.plot([sort_n[0],sort_n[0]],[0.95,1.05],'k--',label='defined by the most extreme blue point')\n",
    "plt.plot([(sort_n[0]-sort_c[-1])/2+sort_c[-1],(sort_n[0]-sort_c[-1])/2+sort_c[-1]],[0.95,1.05],'k-.',label='middle of extreme of two classes')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data are not linearly separable you need to be able to choose support vectors that are going to do some missclassification but for the greater good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are back to our regularization problem and of course **Support Vector Machine has a regularization parameter : C**. The game now becomes placing your threshold right in the middle of points (support vectors) from  each classes that you have \"chosen\" to be general points for decision making : **they don't need to be the two closest points from different classes anymore. They need to be points where your hyperplane makes the least error differentiating classes.**\n",
    "\n",
    "As stated before, SVM is, at its core, very geometrical and I would like to introduce it a little bit more formally before talking about non-linear kernels.\n",
    "\n",
    "**In the case where you are putting your threshold/hyperplane right in the middle of the two closest points from two different classes (which is a really intuitive thing to do), what you are actually doing is finding a particular hyperplane of the form:**\n",
    "\n",
    "$\\overrightarrow{w}.\\overrightarrow{x} - b = 0 $ (or in matricial form if you prefer $w^{T}x-b=0$)\n",
    "\n",
    "which is particular because it is right in the middle and so maximize the distance between this hyperplane and both the points from the two different classes which are close to each other: that distance is actually $\\frac{2}{||\\overrightarrow{w}||}$ with of course the norm depending on the scalar product used.\n",
    "\n",
    "![svm](image/1920px-SVM_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image source : image by wikipedia user Larhmam, distributed under a [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/deed.en).\n",
    "\n",
    "You can see that there is a [dot product](https://en.wikipedia.org/wiki/Dot_product) involved : in the case of a linear hyperplane this dot product is juste the cartesian dot product that you probably use all the time. It allows you to calculate distances between points in that cartesian space or between points and hyperplanes. But you might be familiar with other scalar product : like for example when you proceed to a Fourier decomposition of a function. This particular scalar product acts on functions and so is not really of interest for us... But others exist.\n",
    "\n",
    "**So in principle you could use other definitions of distance between points to answer that classification question**. This is what non-linear SVM does and this is why you can choose different so called kernels as hyperparameters as we will see below :\n",
    "\n",
    "$\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}}$ : cartesian\n",
    "\n",
    "$(\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}})^{d}$ : polynomial degree d\n",
    "\n",
    "$exp(-\\gamma||\\overrightarrow{x_{i}}-\\overrightarrow{x_{j}}||^{2})$ : gaussian radial basis\n",
    "\n",
    "$tanh(\\kappa\\overrightarrow{x_{i}}.\\overrightarrow{x_{j}}+c)$ : hyperbolic tangent\n",
    "\n",
    "**This is really powerful for classification but going non-linear by using a kernel trick prevents you to interpret how your features are massaged to create this classifier... So if you want interpretability and do science rather than engineering keep it linear.**\n",
    "\n",
    "Finally let's look at the analysis part of the problem : what is our loss function here ? (what do we want to optimize over? Remember for the logistic regression we were maximizing likelihood.)\n",
    "\n",
    "You want to maximize the distance between your hyperplane and your support vectors. This distance is $\\frac{2}{||\\overrightarrow{w}||}$. So you want to minimize $||\\overrightarrow{w}||$. \n",
    "\n",
    "So you want to minimize $||\\overrightarrow{w}||$ with the constraint that most of the points on each side of the hyperplane belong to the same class. This translates into minimizing what is called a hinge loss :\n",
    "\n",
    "$\\frac{1}{m}\\Sigma^{m}_{k=1}max(0,1-y_{k}(w^{T}x_{k}-b)) - \\frac{1}{C}\\Sigma^{n}_{i=1}||w_{i}||^{2}$\n",
    "\n",
    ", where \n",
    " * $y_k$ is $-1$ or $1$ depending on the class of the point $k$\n",
    " * the class of point $x_k$ is determined by the SVM using the sign of $(w^{T}x_{k}-b)$ (ie, on which side of the $(w^{T}x_{k}-b)=0$ hyperplane we are).\n",
    "\n",
    "\n",
    "Note that you could also use a L1 regularization but it is not implemented in the function we are going to use.\n",
    "\n",
    "Indeed if most of the data points are well separated in term of class on each side of the hyperplane then\n",
    " * most of the time $y_{k}(w^{T}x_{k}-b) \\geq 1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b)=0$ (that's good for minimizing our loss function), \n",
    " * and a few times $y_{k}(w^{T}x_{k}-b) \\leq -1$ and so $max(0,1-y_{k}(w^{T}x_{k}-b) \\geq 2$ (which is polluting our minimization of the loss function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even though SVM as nothing to do with probablities, we are going to transform the results of our classifier back to probabilities (using logistic regression...) to be able to draw a ROC curve. But again I insist, those are just useful transformations but as actually nothing to do with the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example to visualize SVMC <a class=\"anchor\" id=\"toy-example-svm-c\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making toy data\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=600, centers=[[-0.5,-0.5],[1,1]],cluster_std=[[1,1],[1,1]], random_state=6)\n",
    "\n",
    "\n",
    "plt.scatter(X2[:,0],X2[:,1],c=y2)\n",
    "plt.xlim(min(X2[:,1]),max(X2[:,1]))\n",
    "plt.ylim(min(X2[:,1]),max(X2[:,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_SVM\n",
    "\n",
    "#countour_SVM(X,y,regularization strength,kernel,degree in case of poly kernel,gamma in case of rbf kernel,mult)\n",
    "\n",
    "countour_SVM(X2,y2,100,'linear',2,1,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countour_SVM(X2,y2,0.01,'linear',2,1,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countour_SVM(X2,y2,1,'rbf',2,10,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countour_SVM(X2,y2,1,'rbf',2,1,'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countour_SVM(X2,y2,1,'poly',3,1,'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMC pipeline. <a class=\"anchor\" id=\"svm-c-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X2=data['data']\n",
    "y2=data['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2,y2,\n",
    "                                                   random_state=0,stratify=y2)\n",
    "\n",
    "\n",
    "print(\"fraction of class benign in train\",sum(y_train)/len(y_train),'\\n',\n",
    "      \"fraction of class benign in test\",sum(y_test)/len(y_test),'\\n',\n",
    "      \"fraction of class benign in full\",sum(y2)/len(y2))\n",
    "\n",
    "\n",
    "sco='accuracy'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#### That's the new part. Now we call SVC instead of logistic regression. Scaling now is a must!\n",
    "#### We still put the class weight as before, and we add probablity calculation to be able\n",
    "#### to calculate other score than accuracy, like the AUC.\n",
    "\n",
    "\n",
    "pipeline_SVM=Pipeline([('scalar',StandardScaler()),('model',svm.SVC(class_weight='balanced',probability=True))])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#### Of course the hyperparameters are SVMC specific now.\n",
    "\n",
    "grid_values = {\"model__kernel\": ['linear', 'rbf', 'poly'],\n",
    "                 \"model__C\":np.logspace(-2, 2, 10),\n",
    "                 \"model__degree\":np.arange(0,5,1),\n",
    "                 \"model__gamma\": np.logspace(-2,1,10)}\n",
    "\n",
    "\n",
    "grid_SVM_acc = GridSearchCV(pipeline_SVM, param_grid = grid_values, scoring=sco, n_jobs=-1)\n",
    "\n",
    "grid_SVM_acc.fit(X_train, y_train)\n",
    "\n",
    "y_decision_fn_scores_acc=grid_SVM_acc.score(X_test,y_test)\n",
    "\n",
    "print('Grid best parameter (max.'+sco+'): ', grid_SVM_acc.best_params_)\n",
    "print('Grid best score ('+sco+'): ', grid_SVM_acc.best_score_)\n",
    "print('Grid best parameter (max.'+sco+') model on test: ', y_decision_fn_scores_acc)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_test_c=grid_SVM_acc.predict(X_test)#predict y_test from X_test thanks to your trained model\n",
    "\n",
    "confusion_mc_c = confusion_matrix(y_test, y_pred_test_c)\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_mc_c)\n",
    "df_cm_c = pd.DataFrame(confusion_mc_c, \n",
    "                     index = [i for i in range(2)], columns = [i for i in range(2)])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(df_cm_c, annot=True)\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_test, y_pred_test_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "\n",
    "y_score_SVM_c = grid_SVM_acc.decision_function(X_test)\n",
    "fpr_SVM_c, tpr_SVM_c, thre = roc_curve(y_test, y_score_SVM_c)\n",
    "roc_auc_SVM_c = auc(fpr_SVM_c, tpr_SVM_c)\n",
    "\n",
    "proba=sc.special.expit(thre)\n",
    "for i in range(len(proba)):\n",
    "    if abs(proba[i]-0.5)<0.1:\n",
    "        keep=i\n",
    "        break\n",
    "        \n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_SVM_c, tpr_SVM_c, lw=3, label='SVC ROC curve\\n (area = {:0.2f})'.format(roc_auc_SVM_c))\n",
    "plt.plot(fpr_SVM_c[keep], tpr_SVM_c[keep],'ro',label='threshold=0.5')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (SVM classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "SVM = svm.SVC(kernel='linear',C=0.01,class_weight='balanced',probability=True)\n",
    "SVM.fit(X_train_scaled, y_train)\n",
    "w=SVM.coef_[0]#get the weights\n",
    "\n",
    "sorted_features=sorted([[data[\"feature_names\"][i],abs(w[i])] for i in range(len(w))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for feature, weight in sorted_features:\n",
    "    print('\\t{: <25}\\t{:.3f}'.format(feature,weight) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Regression <a class=\"anchor\" id=\"svm-r\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the same algorithm for regression, with the only difference that this time instead of finding the hyperplane that is the farthest from the support, we find the hyperplane that is the closest from those support. \n",
    "\n",
    "Do it for our diabete data set, just by replacing SVC by SVR. Just use the `'gamma'` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "df_diabetes=pd.DataFrame([[diabetes['data'][j,i] for i in range(len(diabetes['feature_names'])) if diabetes['feature_names'][i]!=\"sex\"] for j in range(diabetes['data'].shape[0])],\n",
    "                         columns=[diabetes['feature_names'][i] for i in range(len(diabetes['feature_names']))if diabetes['feature_names'][i]!=\"sex\"])\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(np.array(df_diabetes), np.array(diabetes['target']),\n",
    "                                                   random_state=0)\n",
    "\n",
    "\n",
    "sco='r2'\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#### Use SVR instead of SVC\n",
    "\n",
    "pipeline_SVR=Pipeline([('scalar',StandardScaler()),('model',svm.SVR())])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#### It was too long to do a big gridsearch so I focused on rbf kernel\n",
    "\n",
    "grid_values = {\"model__kernel\": ['rbf'],\n",
    "                 \"model__C\":np.logspace(-2, 2, 20),\n",
    "                 #\"model__degree\":np.arange(1,3,1),\n",
    "                 \"model__gamma\": np.logspace(-2,2,20)}\n",
    "\n",
    "\n",
    "grid_SVR_acc = GridSearchCV(pipeline_SVR, param_grid = grid_values, scoring=sco, n_jobs=-1)\n",
    "\n",
    "grid_SVR_acc.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "y_decision_fn_scores_acc=grid_SVR_acc.score(X_diabetes_test,y_diabetes_test)\n",
    "\n",
    "print('Grid best parameter (max.'+sco+'): ', grid_SVR_acc.best_params_)\n",
    "print('Grid best score ('+sco+'): ', grid_SVR_acc.best_score_)\n",
    "print('Grid best parameter (max.'+sco+') model on test: ', y_decision_fn_scores_acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately because of the non linear transformation involved in the RBF kernel we can not interpret the fit in terms of the covariables/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_SVR=Pipeline([('scalar',StandardScaler()),('model',svm.SVR())])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "grid_values = {\"model__kernel\": ['poly'],#['linear', 'rbf', 'poly'],\n",
    "                 \"model__C\":np.logspace(-2, 2, 20),\n",
    "                 \"model__degree\":np.arange(0,5,1)}#,\n",
    "                 #\"model__gamma\": np.logspace(-2,1,5)}\n",
    "\n",
    "grid_SVR_acc = GridSearchCV(pipeline_SVR, param_grid = grid_values, scoring=sco,n_jobs=-1)\n",
    "\n",
    "grid_SVR_acc.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "y_decision_fn_scores_acc=grid_SVR_acc.score(X_diabetes_test,y_diabetes_test)\n",
    "print('Grid best parameter (max.'+sco+'): ', grid_SVR_acc.best_params_)\n",
    "print('Grid best score ('+sco+'): ', grid_SVR_acc.best_score_)\n",
    "print('Grid best parameter (max.'+sco+') model on test: ', y_decision_fn_scores_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Decision tree modeling : a (new?) loss function and new ways to do regularization. <a class=\"anchor\" id=\"decision-tree\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple decision tree for classification. <a class=\"anchor\" id=\"simple-tree-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A simple decision tree reduces your problem into a hierarchichal sequence of questions on your features that can be answered by yes or no and that subdivides the data into 2 subgroups on which a new question is asked, and so on and so on.**\n",
    "\n",
    "Ok but a huge number of trees can actually be built just by considering the diffrent orders of questions asked. How does the algorithm deals with this?\n",
    "\n",
    "Quite simply actually. **It tests all the features and chooses the most discriminative (in terms of your labels) feature to divide the data into 2 subsets of data that answered yes or no to the question.** Then on those subsets the algorithm chooses again the most discriminative feature. How do you discriminate? By calculating an **impurity : meaning how much your feature splitting is still having mix classes**. Note that impurity then goes through a function to give a score: either it is a simple Shannon entropy or it is a Gini entropy.\n",
    "\n",
    "Imagine you have a dataset with feature color (red or blue) and feature shape (square or circle), and 2 classes (1,2):\n",
    "\n",
    "If answering `True` to \"feature color is red\" gives you a subgroup of 10 class 1, and 1 class 2 and, `False` gives you 2 class 1, and 11 class 2 then it is a better classifier than:\n",
    "\n",
    "\"feature shape is square\" that gives you a subgroup of 5 class 1, and 7 class 2 on one hand, and 7 class 1 and 5 class 2 on the other hand.\n",
    "\n",
    "![tree](image/Tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The points are better separated in terms of their classes by answering True or False for feature color than for feature shape.** Some of you could already see that entropy calculation could be a good way to summarize all that text above, hence my little aparte about impurity before. This is also why there is a question mark in the title.\n",
    "\n",
    "**This is something \"easy\" to visualize if your features are categorical like 'yellow' but if they are numeric features like weights, how do you transform that to a yes/no questions? By choosing a threshold...** But how do you choose the threshold? Again you find the threshold by choosing the number with the minimum impurity. When you have a threshold for each feature then you can compare their impurity. You will have to compute this threshold at each step of your tree since at each step you are considering different subdatasets.\n",
    "\n",
    "\n",
    "Before going further, just a little bit of vocabulary. Trees are made of nodes (where the question is asked and where the splitting occurs). A branch is the outcome of a splitting. A leaf is the last node on a branch (no more splitting).\n",
    "\n",
    "### Toy example to visualize decision tree. <a class=\"anchor\" id=\"toy-decision-tree\"></a>\n",
    "\n",
    "Let explore some hyperparameters of this method that, you will see in those exemples, act like a regularization:\n",
    "- Max Tree depth: the maximum number of consecutive questions to ask\n",
    "- Min Splitting of nodes: the minimum number of point that should be considered to make a new rule, outside of the leaves\n",
    "- Min Splitting of leaves: the minimum number of point that should be considered to make a new rule at the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=600, centers=[[-0.5,-0.5],[1,1]],cluster_std=[[1,1],[1,1]], random_state=6)\n",
    "\n",
    "plt.scatter(X2[:,0],X2[:,1],c=y2)\n",
    "plt.xlim(min(X2[:,1]),max(X2[:,1]))\n",
    "plt.ylim(min(X2[:,1]),max(X2[:,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for graphical representation of the trees : you don't need it for analysis.\n",
    "#you will need to pip install pydotplus the first time so just remove the # signe\n",
    "#!pip install pydotplus\n",
    "#You will need graphviz lib also. Download it here : https://graphviz.gitlab.io/download/\n",
    "#You will need to add it to your python path\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/graphviz-2.38/release/bin/'#path where you installed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_tree\n",
    "\n",
    "#countour_tree(X,y,criterion for splitting,max depth ,min node split,min leaf split,max_f)\n",
    "\n",
    "countour_tree(X2,y2,'entropy',None,2,1,None)\n",
    "#You can see that there are 5 hyperparameters here. Let's see what they do and what they mean. \n",
    "# I bet you can already guess it is going to be related to regularization....\n",
    "# After X,y you have 'entropy' which is one way to calculate impurity (you could also put gini here), \n",
    "# then the max depth of your tree, \n",
    "# then the number of points that should be concerned by the making of a new rule (splitting of the nodes), \n",
    "# and the number of points that should be considered to make a final leaf classification. \n",
    "# Finally the last one is maximum number of features to consider for making a new rule..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an incredibly complex model. \n",
    "\n",
    "Please, note that since every node is a question asked on one particular feature and features are never directly compared, you don't need scaling! This observation that each question always involves one feature at a time can be also seen in the way the boundaries between classes are made in the graph : there is no diagonal boundaries. You can only see lines parallel to the plot axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max depth: 4 , min number of points to consider : 2 , max number of point for a leaf : 1 , nb feature: default\n",
    "countour_tree(X2,y2,'entropy',4,2,1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countour_tree(X2,y2,'entropy',4,40,1,None)\n",
    "# I don't really have a feeling of where I should stop the tree depth. \n",
    "# But I have an understanding of this other parameter (here set to 40) called min_samples_leaf : \n",
    "#     it sets the minimal number of data points that the overall chain of rules should concern.\n",
    "#     ie., Do you really care of making a whole new set of rules to explain only one particular data point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countour_tree(X2,y2,'entropy',4,40,40,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The interesting thing with this kind of method is that it works with all types of features (numerical or class, I mean in theory), you don't need to rescale and it already includes non linear fitting. Moreover it is 'easy' to interpret.** \n",
    "\n",
    "But....(yes there is a but, there is no free lunch)\n",
    "\n",
    "**Even with all of those hyperparamaters they are still not great on new data (inacuracy....).** We will see that in the real data example below and we will see more powerful technics based on decision trees that are more costly but generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single decision tree pipeline. <a class=\"anchor\" id=\"single-tree-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "df_cancer = pd.concat([pd.DataFrame(cancer['data'],columns=cancer['feature_names']),\\\n",
    "                       pd.DataFrame(cancer['target'],columns=['malignant'])],axis=1)\n",
    "\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = train_test_split(X_cancer, y_cancer,\n",
    "                                                   random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "#### We use DecisionTreeClassifier with the different hyperparameters we want to explore\n",
    "\n",
    "grid_values = {'criterion': ['entropy','gini'],\n",
    "               'max_depth':np.arange(2,len(X_cancer_train),20),\n",
    "               'min_samples_split':np.arange(2,len(X_cancer_train),20),\n",
    "              'min_samples_leaf':np.arange(1,len(X_cancer_train),20)}\n",
    "\n",
    "grid_tree_acc = GridSearchCV(DecisionTreeClassifier(class_weight=\"balanced\"), param_grid = grid_values, scoring='accuracy',n_jobs=-1)\n",
    "grid_tree_acc.fit(X_cancer_train, y_cancer_train)\n",
    "\n",
    "y_decision_fn_scores_acc=grid_tree_acc.score(X_cancer_test,y_cancer_test)\n",
    "\n",
    "print('Grid best parameter (max. accuracy): ', grid_tree_acc.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_tree_acc.best_score_)\n",
    "print('Grid best parameter (max. accuracy) model on test: ', y_decision_fn_scores_acc)\n",
    "\n",
    "y_pred_test_c=grid_tree_acc.predict(X_cancer_test)\n",
    "\n",
    "confusion_mc_c = confusion_matrix(y_cancer_test, y_pred_test_c)\n",
    "df_cm_c = pd.DataFrame(confusion_mc_c, \n",
    "                     index = [i for i in range(0,len(cancer['target_names']))], columns = [i for i in range(0,len(cancer['target_names']))])\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(df_cm_c, annot=True)\n",
    "plt.title('Crit:'+str(grid_tree_acc.best_params_['criterion'])\n",
    "          +' , max_depth:'+str(grid_tree_acc.best_params_['max_depth'])+' , min_split:'+str(grid_tree_acc.best_params_['min_samples_split'])+\n",
    "          ' ,min_leaf:'+str(grid_tree_acc.best_params_['min_samples_leaf'])\n",
    "          +'\\nAccuracy:{0:.3f}'.format(accuracy_score(y_cancer_test, \n",
    "                                                                       y_pred_test_c)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print(classification_report(y_cancer_test, y_pred_test_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an awfully complicated model that is performing similarly to other models trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "tree = DecisionTreeClassifier(criterion=grid_tree_acc.best_params_['criterion'],\n",
    "                             max_depth=grid_tree_acc.best_params_['max_depth'],\n",
    "                             min_samples_leaf=grid_tree_acc.best_params_['min_samples_leaf'],\n",
    "                             min_samples_split=grid_tree_acc.best_params_['min_samples_split'])\n",
    "tree.fit(X_cancer_train, y_cancer_train)\n",
    "w=tree.feature_importances_#get the weights\n",
    "\n",
    "sorted_features=sorted([[cancer['feature_names'][i],abs(w[i])] for i in range(len(w))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for feature, weight in sorted_features:\n",
    "    print('\\t{: <25}\\t{:.3f}'.format(feature,weight) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Random Forest in classification. <a class=\"anchor\" id=\"rf-c\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest algorithm relies on two main concepts : producing/training many trees all different from an element of randomness, and then asking all the trees what their prediction on new points is and choose a way to agglomerate those different predicitions (mainly averaging).** \n",
    "\n",
    "Those elements of randomness concern the **training dataset via bootstrapping methods** and also only using a **random subset of features for creating the trees in the forest** (this is different from setting a max_feature this time!). Each  bootstrap part is going to be used to **train a new tree in the forest** (and this tree will be build only with the randomly drawn subset of parameters).\n",
    "\n",
    "**Bootstrapping methods are sampling methods in which you randomly draw a subsample of your data**. You keep the same size as the initial data though. This is possible because duplicates of points are allowed. This subsample is then used to train a tree which will have also a subsample of the features only. \n",
    "\n",
    "I am sure you can see intuitively how that is going to help the generalization of our model.\n",
    "\n",
    "So now you have all the parameters seen before for individually creating each tree of the forest, but you also have a parameter controlling the number of trees in your forest.\n",
    "\n",
    "\n",
    "**In the following plots I am plotting the result for a random forest algorithm and compare it to a single decision tree sharing the same hyperparameters value than the one used in the random forest**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_RF\n",
    "\n",
    "#countour_RF(X,y,n_tree,crit,maxd,min_s,min_l,max_f)\n",
    "\n",
    "countour_RF(X2,y2,100,'gini',4,40,40,None)\n",
    "#Same as for decision tree except that we have here one more hyperparameter, here\n",
    "# put to 100 and that represents the number of bootstraps \n",
    "# (number of trees trained and then participating to the vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "##### Now we use RandomForestClassifier\n",
    "grid_values = {'criterion': ['entropy','gini'],\n",
    "               'n_estimators':np.arange(1,500,100), \n",
    "               'max_depth':np.arange(2,int(len(X_cancer_train)/10),20),\n",
    "               'min_samples_split':np.arange(2,int(len(X_cancer_train)/10),20),\n",
    "              'min_samples_leaf':np.arange(1,int(len(X_cancer_train)/10),20)}\n",
    "\n",
    "grid_tree_acc = GridSearchCV(RandomForestClassifier(class_weight='balanced'), param_grid = grid_values, scoring='accuracy',n_jobs=-1)\n",
    "\n",
    "grid_tree_acc.fit(X_cancer_train, y_cancer_train)\n",
    "\n",
    "y_decision_fn_scores_acc=grid_tree_acc.score(X_cancer_test,y_cancer_test)\n",
    "\n",
    "print('Grid best parameter (max. accuracy): ', grid_tree_acc.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_tree_acc.best_score_)\n",
    "\n",
    "print('Grid best parameter (max. accuracy) model on test: ', y_decision_fn_scores_acc)\n",
    "\n",
    "y_pred_test_c=grid_tree_acc.predict(X_cancer_test)\n",
    "\n",
    "confusion_mc_c = confusion_matrix(y_cancer_test, y_pred_test_c)\n",
    "df_cm_c = pd.DataFrame(confusion_mc_c, \n",
    "                     index = [i for i in range(0,len(cancer['target_names']))], columns = [i for i in range(0,len(cancer['target_names']))])\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(df_cm_c, annot=True)\n",
    "plt.title('Crit:'+str(grid_tree_acc.best_params_['criterion'])\n",
    "          +' , n_estimators:'+str(grid_tree_acc.best_params_['n_estimators'])\n",
    "          +' , max_depth:'+str(grid_tree_acc.best_params_['max_depth'])+' , min_split:'+str(grid_tree_acc.best_params_['min_samples_split'])+\n",
    "          ' ,min_leaf:'+str(grid_tree_acc.best_params_['min_samples_leaf'])\n",
    "          +'\\nAccuracy:{0:.3f}'.format(accuracy_score(y_cancer_test, \n",
    "                                                                       y_pred_test_c)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_cancer_test, y_pred_test_c))\n",
    "\n",
    "RF = RandomForestClassifier(criterion=grid_tree_acc.best_params_['criterion'],\n",
    "                              n_estimators=grid_tree_acc.best_params_['n_estimators'],\n",
    "                             max_depth=grid_tree_acc.best_params_['max_depth'],\n",
    "                             min_samples_leaf=grid_tree_acc.best_params_['min_samples_leaf'],\n",
    "                             min_samples_split=grid_tree_acc.best_params_['min_samples_split'])\n",
    "RF.fit(X_cancer_train, y_cancer_train)\n",
    "w=RF.feature_importances_#get the weights\n",
    "\n",
    "sorted_features=sorted([[cancer['feature_names'][i],abs(w[i])] for i in range(len(w))],key=itemgetter(1),reverse=True)\n",
    "\n",
    "print('Features sorted per importance in discriminative process')\n",
    "for feature, weight in sorted_features:\n",
    "    print('\\t{: <25}\\t{:.3f}'.format(feature,weight) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Radom Forest in regression. <a class=\"anchor\" id=\"rf-r\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is now the split is made according to the mean square error or mean absolute error, instead of entropy or Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "df_diabetes=pd.DataFrame([[diabetes['data'][j,i] for i in range(len(diabetes['feature_names'])) if diabetes['feature_names'][i]!=\"sex\"] for j in range(diabetes['data'].shape[0])],\n",
    "                         columns=[diabetes['feature_names'][i] for i in range(len(diabetes['feature_names']))if diabetes['feature_names'][i]!=\"sex\"])\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(np.array(df_diabetes), np.array(diabetes['target']),\n",
    "                                                   random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "grid_values = {'criterion': ['mse'],\n",
    "               'n_estimators':np.arange(1,500,50), \n",
    "               'max_depth':np.arange(2,int(len(X_diabetes_train)/2),20),\n",
    "               'min_samples_split':np.arange(2,int(len(X_diabetes_train)/5),20),\n",
    "              'min_samples_leaf':np.arange(1,int(len(X_diabetes_train)/5),20)}\n",
    "\n",
    "grid_tree_acc = GridSearchCV(RandomForestRegressor(), param_grid = grid_values, scoring='r2',n_jobs=-1,cv=3)\n",
    "\n",
    "grid_tree_acc.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "y_decision_fn_scores_acc=grid_tree_acc.score(X_diabetes_test,y_diabetes_test)\n",
    "\n",
    "print('Grid best parameter (max. r2): ', grid_tree_acc.best_params_)\n",
    "print('Grid best score (r2): ', grid_tree_acc.best_score_)\n",
    "print('Grid best parameter (max. r2) model on test: ', y_decision_fn_scores_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Regression exercise : predicting daily maximal temperature <a class=\"anchor\" id=\"exo-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>temp_2</th>\n",
       "      <th>temp_1</th>\n",
       "      <th>average</th>\n",
       "      <th>actual</th>\n",
       "      <th>forecast_noaa</th>\n",
       "      <th>forecast_acc</th>\n",
       "      <th>forecast_under</th>\n",
       "      <th>friend</th>\n",
       "      <th>week_Fri</th>\n",
       "      <th>week_Mon</th>\n",
       "      <th>week_Sat</th>\n",
       "      <th>week_Sun</th>\n",
       "      <th>week_Thurs</th>\n",
       "      <th>week_Tues</th>\n",
       "      <th>week_Wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.6</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>45.7</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>45.8</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>45.9</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>46.0</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year  month  day  temp_2  temp_1  average  actual  \\\n",
       "0           0  2016      1    1      45      45     45.6      45   \n",
       "1           1  2016      1    2      44      45     45.7      44   \n",
       "2           2  2016      1    3      45      44     45.8      41   \n",
       "3           3  2016      1    4      44      41     45.9      40   \n",
       "4           4  2016      1    5      41      40     46.0      44   \n",
       "\n",
       "   forecast_noaa  forecast_acc  forecast_under  friend  week_Fri  week_Mon  \\\n",
       "0             43            50              44      29         1         0   \n",
       "1             41            50              44      61         0         0   \n",
       "2             43            46              47      56         0         0   \n",
       "3             44            48              46      53         0         1   \n",
       "4             46            46              46      41         0         0   \n",
       "\n",
       "   week_Sat  week_Sun  week_Thurs  week_Tues  week_Wed  \n",
       "0         0         0           0          0         0  \n",
       "1         1         0           0          0         0  \n",
       "2         0         1           0          0         0  \n",
       "3         0         0           0          0         0  \n",
       "4         0         0           0          1         0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv('data/One_hot_temp.csv')\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " * year: 2016 for all data points\n",
    " * month: number for month of the year\n",
    " * day: number for day of the year\n",
    " * week: day of the week as a character string\n",
    " * temp_2: max temperature 2 days prior\n",
    " * temp_1: max temperature 1 day prior\n",
    " * average: historical average max temperature\n",
    " * actual: max temperature measurement\n",
    " * friend: your friend’s prediction, a random number between 20 below the average and 20 above the average\n",
    " \n",
    "\n",
    "Additionally, all the features noted forecast are weather forecast given by some organisation for that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We want to predict `actual`, th actual max temperature of a day.\n",
    "\n",
    "Use a random forest to do so. You can inspire yourself from the examples of code above.\n",
    "\n",
    "Here are a couple of plots to get you started with the data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "feature_list=list(features.columns)\n",
    "labels=features[\"actual\"]\n",
    "# Dates of training values\n",
    "months = np.array(features)[:, feature_list.index('month')]\n",
    "days = np.array(features)[:, feature_list.index('day')]\n",
    "years = np.array(features)[:, feature_list.index('year')]\n",
    "\n",
    "# List and then convert to datetime object\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})\n",
    "\n",
    "\n",
    "plt.xlabel('Date'); \n",
    "plt.ylabel('Maximum Temperature (F)')\n",
    "\n",
    "# Plot the actual values\n",
    "plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')\n",
    "plt.xticks(rotation = '60');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "feature_list=list(features.columns)\n",
    "labels=features[\"average\"]\n",
    "# Dates of training values\n",
    "months = np.array(features)[:, feature_list.index('month')]\n",
    "days = np.array(features)[:, feature_list.index('day')]\n",
    "years = np.array(features)[:, feature_list.index('year')]\n",
    "\n",
    "# List and then convert to datetime object\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'average': labels})\n",
    "\n",
    "\n",
    "plt.xlabel('Date'); \n",
    "plt.ylabel('Maximum Temperature (F)')\n",
    "\n",
    "# Plot the average values\n",
    "plt.plot(true_data['date'], true_data['average'], 'b-', label = 'average')\n",
    "plt.xticks(rotation = '60');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - Read in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-5 solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 8-17 solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - setup and fit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19-34 solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 36-40 solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - get the feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 41-49 solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - using permutation to get the importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 50-73 solutions/solution_03_02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - BONUS - refit with a smaller and better feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_02bis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - BONUS - re-thinking the splitting strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_02ter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - BONUS - an even better splitting strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03_02quat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Back to the ToC](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# Annexes <a id='annex'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df=sns.load_dataset(\"iris\")\n",
    "# Here we use the data loader from seaborn but such data loaders also exist with scikit-learn and are more generally delt\n",
    "#with the dataframe handler pandas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue=\"species\")\n",
    "# Seaborn allows you to 'split' your data according to a chosen parameter hue. Here I chose to color split the data according\n",
    "#to the target\n",
    "#description diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you get from the plots above?\n",
    "\n",
    "Looking at the diagonal of these plots : petal features separate the species more efficiently than sepal features.\n",
    "\n",
    "There is a very strong correlation between `petal_length` and `petal_width` : those two features are probably so similar that keeping them both could be redundant.\n",
    "\n",
    "The least correlation visible seems to be between `sepal_width` and all the others.\n",
    "\n",
    "By itself `sepal_width` is not good at differentiating species but associated with other features we can already see groups forming by species. And since they are very much non-colinear I would say that, in dimension two, `petal_length` and `sepal_width` are already a good pick for low dimensions models.\n",
    "\n",
    "You can actually quantify the correlation between features by calling the `corr()` function in pandas. You would prefer (and sometime is requiered) having a subset of features that are not correlated to each others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_corr = pd.DataFrame(df.corr(),columns=list(df.columns)[:-1])\n",
    "# pca.components_ : recovering the matrix that describes the principal component in the former feature basis\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(df_corr,cmap='plasma')\n",
    "plt.yticks(np.arange(0+0.5,4+0.5,1),[v for v in df_corr.columns],rotation=0)\n",
    "plt.title(\"Pearson correlations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "One thing (among others) that you can do is to look for a **subset of features that seems to be important to describe the target class**. It's like the pairplots above but instead of just looking at it you choose the features you want to keep.\n",
    "\n",
    "You can choose different metrics for 'how important to describe the class' a feature is. \n",
    "Many of those metrics utilize concepts that we haven't introduced yet, in contexts that we haven't seen yet, so I will introduce two metrics for classification that don't need too much of *a priori* knowledge. \n",
    "\n",
    "`Scikit-learn` lets you specify a threshold on the features are kept, either as:\n",
    "* a direct number: `SelectKBest`.\n",
    "* important features from a percentile of your top importance score: `SelectPercentile`.\n",
    "* an error type: `SelectFpr` or `SelectFdr` (see course 2 logistic regression part).\n",
    "\n",
    "\n",
    "`Scikit-learn` offers you different scores to calculate the importance of your features.\n",
    "\n",
    "* **ANOVA-F** : F=$\\frac{Var_{feature\\_i}(Between\\_class)}{Var_{feature\\_i}(Within\\_class)}$. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**F** itself gives you how much a feature $i$ variance is different between classes, normalized by the intrinsic variance of that feature per class. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "So if **F** is big it means that the variation that you observe between classes is big compared to the variance of this feature : \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "it behaves differently for different classes so it it is a good feature to keep for the classification. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "To this **F** is associated a **p-value** that you would use for scoring.\n",
    "\n",
    "\n",
    "* **Chi2** ($\\chi^{2}$) test. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "You suppose the null hypothesis that this feature $i$ is homogenously distributed among classes\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "and so you are expecting that its representation in different classes should be very similar to what you can calculate for the bulk data\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    " i.e. $\\frac{\\Sigma^{n\\_points} feature_{i}}{n\\_points}$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "You then compare the actual distribution of this feature in different classes to your null model predictions. If this **sum of square differences**: \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\Sigma^{n\\_class}_{k}\\frac{(expected\\_form\\_null\\_hypothesis_{k}-observed_{k})^{2}}{observed}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "is big then the null hypothesis has to be rejected and this feature is significant for classifying. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The sum of these square quantities over the different classes asymptotically follows a $\\chi^{2}$ \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "distribution and thus you have access to a **p-value for scoring**.\n",
    "\n",
    "\n",
    "Another score would be to use the amount of [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) shared between a feature and our target. \n",
    "\n",
    "The way this mutual information is caclulated is out of the scope of this class as it is a bit technical.\n",
    "\n",
    "##### For regression just use correlation or Mutual Iformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "skb = SelectKBest(chi2, k=2)#creating the object SelectKBest and settling for 2 best features (k=2) in term of chi2 score\n",
    "skb.fit(df[list(df.columns)[:-1]], df[list(df.columns)[-1]])#calculating the chi2 for each features\n",
    "\n",
    "dico_pval={df.columns[i]:v for i,v in enumerate(skb.pvalues_)}\n",
    "print(\"features Chi2 scores (p-values):\")#all the features and the chi2 pvalues associated. use .pvalues_\n",
    "for feature,pval in dico_pval.items() :\n",
    "    print('\\t',feature , ':' , pval )\n",
    "\n",
    "X_new=skb.transform(df[list(df.columns)[:-1]])# keep only the k=2 best features according to the score\n",
    "\n",
    "print(\"New data with only the k=2 best features kept :\")\n",
    "print(X_new[:5,]) #printing only the 5 first entries\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X3, y3 = make_blobs(n_samples=120, centers=3,cluster_std=3, random_state=6)# 120 points, 3 blobs/clusters with some spread=3\n",
    "#Random_state is here just to be sure that every time you will get the same blobs. If you change the random_state or do not\n",
    "#specify it then you will get a new plot every time you call the function (random seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course all of that can be applied to a multi-classes classification. How is it tipically done?\n",
    "\n",
    "There are many different ways of tackling the problem, that end up being a combination of these 4 elements :\n",
    "\n",
    "- **Either you treat the problem as one class vs one class**.\n",
    "\n",
    "- **Or you treat the problem as a one class vs the rest : you subdivide the problem into three different problems either your are class 1 and you consider the other classes as being one big class \"non 1\", and you do the same for the other class**.\n",
    "\n",
    "In any case you need to decide **how you are going to agglomerate those different statistics (different ROC curves for example)**:\n",
    "\n",
    "-You can either do a **micro average** where you pull all your statistics together (number of FP and so on for every sub problem) and then caluclate your statistic like the TPR\n",
    "\n",
    "-Or you can go for a **macro average** : where you calculate the satistics for each sub problem and then do the average.\n",
    "\n",
    "\n",
    "\n",
    "Think about the differences induced by those metrics. Why should you use one more than the other? Or maybe you should always use all of them?\n",
    "\n",
    "Spoiler it has to do with overall separability and balance between the different class.\n",
    "\n",
    "\n",
    "What strategy your logistic regression uses so you can plot the right curves, is a tricky question. For a first pass on your data always set the multiclasses method to be ovr (one vs rest) : understanding the hyperplanes relation to decision probability and the ROC curve is more intuitive that way, and I believe less sensitive to imbalance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_lr_more \n",
    "#one vs rest implementation\n",
    "countour_lr_more('l2',X3,y3,10,'ovr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import countour_lr_more \n",
    "#softmax implementation (something only available with logistic regression), and ahgain different from one vs one\n",
    "#and one vs rest\n",
    "countour_lr_more('l2',X3,y3,10,'multinomial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py38)",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
